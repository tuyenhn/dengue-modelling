---
output: 
  html_document:
    df_print: paged
    toc: yes
    theme: journal
---

# Library

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(feasts)
library(tsibble)
library(fable.prophet)
library(fabletools)
library(fable)
library(lubridate)
library(ggdist)
```

# Data ingestion

## Incidence data

```{r}
incidence_raw <- read_csv("../incidence_ts_in.csv", show_col_types = FALSE) %>%
  mutate(date_admitted = as.Date(date_admitted)) %>%
  as_tsibble(index = date_admitted) %>%
  fill_gaps(n = 0)

incidence_monthly_df <- incidence_raw %>%
  index_by(agg = ~ yearmonth(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg)

train_monthly_df <- incidence_monthly_df %>%
  filter(year(date_admitted) < 2019)
```

# Time series analysis

```{r}
p_base <- ggplot(train_monthly_df, aes(x = date_admitted)) +
  scale_x_yearmonth(date_labels = "%Y", date_breaks = "2 years")

p_base + geom_line(aes(y = n))
```

ARIMA is based on the assumption that the time series is homoscedastic (has constant variance). We can use the White test (White, 1980), to check this. If the time series is not homoscedastic (heteroscedastic), we will perform a Box-Cox transformation.

```{r}
skedastic::white(lm(n ~ date_admitted, data = train_monthly_df))
```

The very low p-value means that we can reject the null hypothesis of homoscedasticity, hence suggests heteroscedasticity in the time series. This means we will need to perform Box-Cox transformation next.

## Box-cox transformation

```{r}
boxcox_mle <- unclass(MASS::boxcox(lm(train_monthly_df$n ~ 1))) %>% as_tibble()

boxcox_white_df <- map(boxcox_mle$x, \(lambda) {
  tibble(
    lambda = lambda,
    transformed_x = list(box_cox(train_monthly_df$n, lambda)),
    white_pvalue = skedastic::white(
      lm(unlist(transformed_x) ~ train_monthly_df$date_admitted)
    )$p.value
  )
}) %>%
  list_c()

boxcox_white_df %>%
  left_join(boxcox_mle, by = c("lambda" = "x")) %>%
  ggplot(aes(x = lambda)) +
  geom_line(aes(y = white_pvalue), color = "red") +
  geom_line(aes(y = (y - min(y)) / 2000), color = "blue") +
  scale_y_continuous(
    "White test p-value",
    sec.axis = sec_axis(trans = ~ . * 2000 + min(boxcox_mle$y), name = "log-Likelihood")
  )
```

```{r}
white_lambda <- boxcox_white_df %>%
  filter(white_pvalue == max(white_pvalue)) %>%
  pull(lambda)
boxcox_lambda <- boxcox_mle %>%
  filter(y == max(y)) %>%
  pull(x)
guerrero_lambda <- train_monthly_df %>%
  features(n, features = guerrero) %>%
  pull(lambda_guerrero)

white_lambda
boxcox_lambda
guerrero_lambda
```

With the tests saying the lambda for Box-Cox transformation that minimizes homoscedasticity is around -0.1 and 0. We will go with a lambda of 0, meaning performing a log transformation, as it's easier for prediction inference in the future.


```{r}
train_monthly_df %<>% mutate(n = log(n))
incidence_monthly_df %<>% mutate(n = log(n))
```

```{r}
p_base + geom_line(aes(y = log(n)))
```


## Unit root test

"One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required." - Hyndman, R.J., & Athanasopoulos, G. (2021). One of the many unit root test is called KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test, Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). For this test, the null hypothesis is stationarity, i.e. smaller p-value suggests the time series is non-stationary and differencing is needed.

```{r}
train_monthly_df %>% features(n, unitroot_ndiffs)
train_monthly_df %>% features(n, unitroot_nsdiffs)
```

This means, 1 non-seasonal differencing and 1 seasonal differencing is suggested.

## Classical decomposition

"The classical decomposition [...] is a relatively simple procedure, and forms the starting point for most other methods of time series decomposition. There are two forms of classical decomposition: an additive decomposition and a multiplicative decomposition."

"The additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate. [...] When a log transformation has been used, this is equivalent to using a multiplicative decomposition on the original data"

Since we already applied log transformation on the data, we will use additive decomposition for analysis below.

```{r}
train_monthly_df %>%
  mutate(n = difference(n, 1)) %>%
  model(
    classic_add = classical_decomposition(n, type = "additive"),
  ) %>%
  components() %>%
  autoplot()

train_monthly_df %>%
  mutate(n = difference(n, 12)) %>%
  model(
    classic_add = classical_decomposition(n, type = "additive"),
  ) %>%
  components() %>%
  autoplot()
```

# Model fitting

```{r}
train_monthly_df %>%
  gg_tsdisplay(
    difference(n),
    plot_type = "partial",
    lag = 12
  )

train_monthly_df %>%
  gg_tsdisplay(
    difference(n, 12),
    plot_type = "partial",
    lag = 48
  )
```

```{r}
future::plan(future::multisession)

fit_all <- train_monthly_df %>%
  model(
    auto_arima = ARIMA(n ~ pdq(1, 0, 1) + PDQ(0, 1, 1)),

    # no seasonal component, AR(1) because of spike at lag-1 in PACF, 1 differencing
    arima110_010 = ARIMA(n ~ pdq(1, 1, 0) + PDQ(0, 1, 0)),

    # Johansson et al. (2016) (no MA)
    arima100_310 = ARIMA(n ~ pdq(1, 0, 0) + PDQ(3, 1, 0)),

    # Facebook Prophet - period = number of obs in each season, order = 10 for annual seasonality
    prophet_multi = prophet(n ~ season(period = 12, order = 10, type = "multiplicative")),
    prophet_addi = prophet(n ~ season(period = 12, order = 10, type = "additive"))
  )

future::plan(future::sequential())
```

# Model evaluation & selection

-   AIC lower is better (less information loss)

```{r}
report(fit_all)

models <- fit_all %>% pivot_longer(
  everything(),
  names_to = "Model name",
  values_to = "Orders"
)
models

# Ljung-Box test, higher means cannot reject null hypothesis of independently distributed data
augment(fit_all) %>%
  features(.innov, ljung_box, lag = 24)

fabletools::accuracy(fit_all)

# ARIMA(1,0,1)(0,1,1)[12] has lowest AIC(c)
fit <- fit_all %>% select(auto_arima)

fit %>%
  gg_tsresiduals(lag = 48)

fit %>%
  residuals() %>%
  pull(.resid) %>%
  shapiro.test()
```

### Raw

```{r}
# temp_train <- incidence_raw %>%
#   index_by(agg = ~ yearmonth(.)) %>%
#   summarise(n = sum(n)) %>%
#   rename(date_admitted = agg)
#
# temp_fit <- temp_train %>%
#   filter(year(date_admitted) < 2019) %>%
#   model(auto_arima = ARIMA(n ~ pdq(1, 0, 2) + PDQ(0, 1, 1)))
#
# temp_forecast <- temp_fit %>%
#   forecast(h = 12) %>%
#   as_tibble() %>%
#   mutate(.mean = log(.mean)) %>%
#   select(date_admitted, .mean)
#
# temp_forecast %>% ggplot(aes(x = date_admitted)) +
#   geom_line(data = temp_train, aes(y = log(n)), color = "black") +
#   geom_line(aes(y = .mean), color = "blue") +
#   scale_x_yearmonth(
#     date_labels = "%Y", date_breaks = "1 year",
#     limits = c(as.Date("2018-01-01"), as.Date("2023-01-01"))
#   )
#
# Metrics::rmse(
#   temp_train %>%
#     filter(year(date_admitted) == 2019) %>% pull(n) %>% log(),
#   temp_forecast$.mean
# )
#
# Metrics::mae(
#   temp_train %>%
#     filter(year(date_admitted) == 2019) %>% pull(n) %>% log(),
#   temp_forecast$.mean
# )
```

## Residual against predictor

```{r}
train_monthly_df %>%
  as_tibble() %>%
  left_join(residuals(fit), by = "date_admitted") %>%
  pivot_longer(n) %>%
  ggplot(aes(x = value, y = .resid)) +
  geom_point()
```

## Residual against fitted

```{r}
augment(fit) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point()
```

# Forecasting

```{r warning=FALSE}
forecasted <- forecast(fit_all, h = 48)

p_forecast <- forecasted %>%
  autoplot(incidence_monthly_df) +
  scale_x_yearmonth(
    date_labels = "%Y", date_breaks = "1 year",
    limits = c(as.Date("2018-01-01"), as.Date("2023-01-01"))
  )

p_forecast

p_forecast +
  facet_wrap(~.model, scales = "free_y")
```

## Accuracy

```{r}
accuracy(forecasted, incidence_monthly_df)
```

## 2019 only

Use fitted models to forecast for 2019 only

```{r}
train_monthly_df_2019 <- incidence_monthly_df %>%
  filter(year(date_admitted) == 2019)

forecasted_2019 <- forecast(fit_all, h = 12)

p_forecast_2019 <- forecasted_2019 %>%
  autoplot(train_monthly_df_2019) +
  scale_x_yearmonth(date_breaks = "3 months", date_labels = "%b", date_minor_breaks = "1 month")

p_forecast_2019

p_forecast_2019 +
  facet_wrap(~.model)

accuracy(forecasted_2019, train_monthly_df_2019)
```

## One-month ahead forecasting

```{r}
fit <- fit_all %>% select(auto_arima)

forecast_1m_df <- train_monthly_df %>% mutate(ci = NA, col = as.factor(0))

for (i in 1:12) {
  forecasted_1m <- fit %>%
    forecast(h = 1) %>%
    as_tibble() %>%
    select(date_admitted, .mean, n) %>%
    rename(ci = n, n = .mean) %>%
    mutate(col = as.factor(1))

  forecast_1m_df %<>% bind_rows(forecasted_1m)

  fit %<>% refit(forecast_1m_df, reestimate = FALSE)
}



ggplot(mapping = aes(x = date_admitted)) +
  geom_line(
    data = train_monthly_df,
    aes(y = n)
  ) +
  stat_lineribbon(
    data = forecast_1m_df,
    aes(ydist = ci),
    linewidth = .5,
    .width = c(0.8, 0.95),
    color = "blue"
  ) +
  scale_fill_brewer()
# +
# scale_x_yearmonth(limit = c(as.Date("2018-01-01"), as.Date("2020-01-01")))
```


```{r}
# one_step_ahead_forecast <- function(
#     model,
#     horizon = 48,
#     same_window_sz = TRUE,
#     reestimation = FALSE,
#     only_2019_test = FALSE) {
#   forecast_1m_df <- train_monthly_df %>% mutate(col = 0)
#
#   for (i in 1:horizon) {
#     forecasted_1m <- fit %>%
#       forecast(h = 1) %>%
#       as_tibble() %>%
#       select(date_admitted, .mean, n) %>%
#       rename(ci = n, n = .mean) %>%
#       mutate(col = as.factor(1))
#
#     forecast_1m_df %<>% bind_rows(forecasted_1m)
#
#     fit %<>% refit(forecast_1m_df, reestimate = FALSE)
#   }
#
#   forecast_1m_df %>%
#     ggplot(aes(x = date_admitted, y = n, col = col)) +
#     geom_line() +
#     geom_line(data = incidence_monthly_df %>% mutate(col = as.factor(0)))
#
#   if (only_2019) {
#     test_set <- incidence_monthly_df %>%
#       filter(year(date_admitted) == 2019) %>%
#       pull(n)
#     forecasted_set <- forecast_1m_df %>%
#       filter(year(date_admitted) == 2019) %>%
#       pull(n)
#   } else {
#     test_set <- incidence_monthly_df %>%
#       filter(year(date_admitted) > 2018) %>%
#       pull(n)
#     forecasted_set <- forecast_1m_df %>%
#       filter(year(date_admitted) > 2018) %>%
#       pull(n)
#   }
#
#   list(
#     sprintf(
#       "arima_1m_ahead_%s_%s",
#       ifelse(reestimation, "reestimate", "no_reestimate"),
#       ifelse(same_window_sz, "no_truncated", "truncated")
#     ),
#     sqrt(mean((test_set - forecasted_set)^2)),
#     mean(abs(test_set - forecasted_set))
#   )
# }
```

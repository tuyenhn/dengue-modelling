# Library

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(feasts)
library(tsibble)
library(fpp3)
```

# Data ingestion
```{r}
incidence_df <- read_csv("../incidence_ts_in.csv", show_col_types = FALSE) %>%
  mutate(date_admitted = as.Date(date_admitted)) %>%
  filter(year(date_admitted) < 2019) %>%
  as_tsibble(index = date_admitted)

incidence_df %<>% fill_gaps(n = 0) %>%
  index_by(agg = ~ yearmonth(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg)

incidence_df_weekly <- read_csv("../incidence_ts_in.csv", show_col_types = FALSE) %>%
  mutate(date_admitted = as.Date(date_admitted)) %>%
  as_tsibble(index = date_admitted) %>%
  fill_gaps(n = 0) %>%
  index_by(agg = ~ yearweek(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg) %>%
  mutate(n = log(n))

incidence_df
```

# Time series analysis
Since starts of epidemics have an exponential growth rate, we apply a logarithmic transformation to the time series.
```{r}
ggplot(incidence_df, aes(x = date_admitted, y = n)) +
  geom_line()

ggplot(incidence_df, aes(x = date_admitted, y = log(n))) +
  geom_line()
```
We can check if we're correct using by checking Box-Cox transformations log-likelihood along lambda from range -2 to 2, along with the Shapiro-Wilk normality test (higher p-value means cannot reject null hypothesis of normal distribution).
## Box-cox transformation
```{r}
boxcox_mle <- unclass(MASS::boxcox(lm(incidence_df$n ~ 1))) %>% as_tibble()

map(boxcox_mle$x, \(lambda) {
  tibble(
    lambda = lambda,
    transformed_x = list(box_cox(incidence_df$n, lambda)),
    normal_pvalue = shapiro.test(unlist(transformed_x))$p.value
  )
}) %>%
  list_c() %>%
  left_join(boxcox_mle, by = c("lambda" = "x")) %>%
  ggplot(aes(x = lambda)) +
  geom_line(aes(y = normal_pvalue), color = "red") +
  geom_line(aes(y = (y - min(y)) / 25000), color = "blue") +
  scale_y_continuous(
    "Normality test p-value",
    sec.axis = sec_axis(trans = ~ . * 25000 + min(boxcox_mle$y), name = "log-Likelihood")
  )
```

With the confirmation from the Box-Cox transformations and normality test, we apply logarithmic transformation on the data.
```{r}
incidence_df %<>% mutate(n = log(n))
```


## Unit root test
"One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required." - Hyndman, R.J., & Athanasopoulos, G. (2021) 
One of the many unit root test is called KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test, Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). For this test, the null hypothesis is stationarity, i.e. smaller p-value suggests the time series is non-stationary and differencing is required.
```{r}
incidence_df %>% features(n, unitroot_kpss)
incidence_df %>%
  mutate(n = difference(n)) %>%
  features(n, unitroot_kpss)

# number of differencing needed
incidence_df %>% features(n, unitroot_ndiffs)
```
This means, at most, 1 differencing is required.


## Classical additive decomposition
```{r}
decomped <- incidence_df %>%
  model(
    classic_add = classical_decomposition(n, type = "additive")
  ) %>%
  components()

decomped %>% autoplot()
```

## Trend-cycle
```{r}
decomped %>%
  as_tsibble() %>%
  autoplot(n, colour = "gray") +
  geom_line(aes(y = trend), color = "red")
```

## Seasonally adjusted
Remove the seasonality from the time series, i.e. dengue is not a seasonal epidemic.

```{r}
decomped %>%
  as_tsibble() %>%
  autoplot(n, colour = "gray") +
  geom_line(aes(y = season_adjust), colour = "#0072B2")
```

# Model fitting
## (S)ARIMA

```{r}
incidence_df %>%
  gg_tsdisplay(
    difference(n, 12),
    plot_type = "partial",
    lag = 36
  )
```

```{r}
fit <- incidence_df %>%
  filter(year(date_admitted) < 2019) %>%
  model(
    auto_arima = ARIMA(n ~ pdq(1, 0, 0) + PDQ(0, 1, 1)),

    # no seasonal component, AR(1) because of spike at lag-1 in PACF, 1 differencing
    arima110_000 = ARIMA(n ~ pdq(1, 1, 0) + PDQ(0, 0, 0)),

    # AR(1) because of spike at lag-1 of every start of season. 1 seasonal differencing, no seasonal AR.
    arima100_010 = ARIMA(n ~ pdq(1, 0, 0) + PDQ(0, 1, 0)),
  )
```


# Model selection & evaluation
* AIC lower is better (less information loss)
```{r}
report(fit)

# ARIMA(1,0,0)(0,1,1)[12] hasl lowest AIC(c)
fit %>% pivot_longer(
  everything(),
  names_to = "Model name",
  values_to = "Orders"
)

fit %<>% select(auto_arima)

fit %>%
  gg_tsresiduals(lag = 36)

# Ljung-Box test, higher means cannot reject null hypothesis of independently distributed data
augment(fit) %>%
  features(.innov, ljung_box, lag = 12, dof = 4)
```

## Residual against predictor
```{r}
incidence_df %>%
  as_tibble() %>%
  left_join(residuals(fit), by = "date_admitted") %>%
  pivot_longer(n) %>%
  ggplot(aes(x = value, y = .resid)) +
  geom_point()
```

## Residual against fitted
```{r}
augment(fit) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point()
```


# Forecasting
```{r}
incidence_df_full <- read_csv("../incidence_ts_in.csv", show_col_types = FALSE) %>%
  mutate(date_admitted = as.Date(date_admitted)) %>%
  as_tsibble(index = date_admitted) %>%
  fill_gaps(n = 0) %>%
  index_by(agg = ~ yearmonth(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg) %>%
  mutate(n = log(n))
```


```{r}
forecast(fit, h = 48) %>%
  filter(.model == "auto_arima") %>%
  autoplot(incidence_df_full)
```

---
output: 
  html_document:
    df_print: paged
---


# Library

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(feasts)
library(tsibble)
library(fpp3)
library(fable.prophet)
```

# Data ingestion

## Incidence data

```{r}
incidence_raw <- read_csv("../incidence_ts_in.csv", show_col_types = FALSE) %>%
  mutate(date_admitted = as.Date(date_admitted)) %>%
  as_tsibble(index = date_admitted) %>%
  fill_gaps(n = 0)

incidence_df_full <- incidence_raw %>%
  index_by(agg = ~ yearmonth(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg)

incidence_df <- incidence_df_full %>%
  filter(year(date_admitted) < 2019)

incidence_df_full_weekly <- incidence_raw %>%
  index_by(agg = ~ yearweek(.)) %>%
  summarise(n = sum(n)) %>%
  rename(date_admitted = agg)

incidence_df_weekly <- incidence_df_full_weekly %>%
  filter(year(date_admitted) < 2019)

incidence_df_weekly
```

# Time series analysis

Since starts of epidemics have an exponential growth rate, we apply a logarithmic transformation to the time series.

```{r}
p_base <- ggplot(incidence_df_weekly, aes(x = date_admitted)) +
  scale_x_yearweek(date_labels = "%Y", date_breaks = "2 years")

p_base + geom_line(aes(y = n))
p_base + geom_line(aes(y = log(n)))
```


We can check if we're correct using by checking Box-Cox transformations log-likelihood along lambda from range -2 to 2, along with the Shapiro-Wilk normality test (higher p-value means cannot reject null hypothesis of normal distribution).

## Box-cox transformation

```{r}
boxcox_mle <- unclass(MASS::boxcox(lm(incidence_df_weekly$n ~ 1))) %>% as_tibble()

map(boxcox_mle$x, \(lambda) {
  tibble(
    lambda = lambda,
    transformed_x = list(box_cox(incidence_df_weekly$n, lambda)),
    normal_pvalue = shapiro.test(unlist(transformed_x))$p.value
  )
}) %>%
  list_c() %>%
  left_join(boxcox_mle, by = c("lambda" = "x")) %>%
  ggplot(aes(x = lambda)) +
  geom_line(aes(y = normal_pvalue), color = "red") +
  geom_line(aes(y = (y - min(y)) / 5e9), color = "blue") +
  scale_y_continuous(
    "Normality test p-value",
    sec.axis = sec_axis(trans = ~ . * 5e9 + min(boxcox_mle$y), name = "log-Likelihood")
  )
```

### QQ plot
```{r}
boxcox_lambda <- boxcox_mle %>%
  filter(y == max(y)) %>%
  pull(x)

tibble(
  index = 1:length(incidence_df_weekly$n),
  precise_lambda = (incidence_df_weekly$n^boxcox_lambda - 1) / boxcox_lambda,
  log_transformed = log(incidence_df_weekly$n),
  sqrt_transformed = sqrt(incidence_df_weekly$n)
) %>%
  pivot_longer(-index, names_to = "method", values_to = "y") %>%
  ggplot(aes(sample = y, color = method)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~method)
```


With the confirmation from the Box-Cox transformations and normality test (even though the p-value is very low), we apply logarithmic transformation on the data.

```{r}
incidence_df %<>% mutate(n = log(n))
incidence_df_full %<>% mutate(n = log(n))
incidence_df_weekly %<>% mutate(n = log(n))
incidence_df_full_weekly %<>% mutate(n = log(n))
```

## Unit root test

"One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required." - Hyndman, R.J., & Athanasopoulos, G. (2021). One of the many unit root test is called KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test, Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). For this test, the null hypothesis is stationarity, i.e. smaller p-value suggests the time series is non-stationary and differencing is required.

```{r}
incidence_df_weekly %>% features(n, unitroot_kpss)
incidence_df_weekly %>%
  mutate(n = difference(n)) %>%
  features(n, unitroot_kpss)

# number of differencing needed
incidence_df_weekly %>% features(n, unitroot_ndiffs)
```

This means, at most, 1 differencing is required.

## Classical decomposition
"The classical decomposition [...] is a relatively simple procedure, and forms the starting point for most other methods of time series decomposition. There are two forms of classical decomposition: an additive decomposition and a multiplicative decomposition."

"The additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate. [...] When a log transformation has been used, this is equivalent to using a multiplicative decomposition on the original data"

Since we already applied log transformation on the data, we will use additive decomposition for analysis below.
```{r}
decomped <- incidence_df_weekly %>%
  model(
    classic_add = classical_decomposition(n, type = "additive")
  ) %>%
  components()

decomped %>% autoplot()
```

## Trend-cycle

```{r}
decomped %>%
  as_tsibble() %>%
  autoplot(n, colour = "gray") +
  geom_line(aes(y = trend), color = "red")
```

## Seasonally adjusted

Remove the seasonality from the time series, i.e. dengue is not a seasonal epidemic.

```{r}
decomped %>%
  as_tsibble() %>%
  autoplot(n, colour = "gray") +
  geom_line(aes(y = season_adjust), colour = "#0072B2")
```

# Model fitting

```{r}
incidence_df_weekly %>%
  gg_tsdisplay(
    difference(n, 52),
    plot_type = "partial",
    lag = 102
  )
```

```{r}
incidence_df_weekly %>%
  filter(year(date_admitted) < 2019) %>%
  model(auto_arima = ARIMA(n ~ pdq(2, 0, 1) + PDQ(1, 1, 0)))

fit_all <- incidence_df_weekly %>%
  filter(year(date_admitted) < 2019) %>%
  model(
    auto_arima = ARIMA(n ~ pdq(1, 0, 1) + PDQ(0, 1, 1)),

    # no seasonal component, AR(1) because of spike at lag-1 in PACF, 1 differencing
    arima110_010 = ARIMA(n ~ pdq(1, 1, 0) + PDQ(0, 1, 0)),

    # AR(1) because of spike at lag-1 of every start of season. 1 seasonal differencing, no seasonal AR.
    arima100_010 = ARIMA(n ~ pdq(1, 0, 0) + PDQ(0, 1, 0)),

    # Johansson et al. (2016) (no MA)
    arima100_310 = ARIMA(n ~ pdq(1, 0, 0) + PDQ(3, 1, 0)),

    # Facebook Prophet - period = number of obs in each season, order = 10 for annual seasonality
    prophet_multi = prophet(n ~ season(period = 12, order = 10, type = "multiplicative")),
    prophet_addi = prophet(n ~ season(period = 12, order = 10, type = "additive"))
  )
```

# Model evaluation & selection

-   AIC lower is better (less information loss)

```{r}
report(fit_all)

models <- fit_all %>% pivot_longer(
  everything(),
  names_to = "Model name",
  values_to = "Orders"
)
models

# Ljung-Box test, higher means cannot reject null hypothesis of independently distributed data
augment(fit_all) %>%
  features(.innov, ljung_box, lag = 12, dof = 4)

accuracy(fit_all)

# ARIMA(1,0,1)(0,1,1)[12] has lowest AIC(c)
fit <- fit_all %>% select(auto_arima)

fit %>%
  gg_tsresiduals(lag = 36)
```

## Residual against predictor

```{r}
incidence_df %>%
  as_tibble() %>%
  left_join(residuals(fit), by = "date_admitted") %>%
  pivot_longer(n) %>%
  ggplot(aes(x = value, y = .resid)) +
  geom_point()
```

## Residual against fitted

```{r}
augment(fit) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point()
```

# Forecasting

```{r warning=FALSE}
forecasted <- forecast(fit_all, h = 48)

p_forecast <- forecasted %>%
  autoplot(incidence_df_full) +
  scale_x_yearmonth(
    date_labels = "%Y", date_breaks = "1 year",
    limits = c(as.Date("2018-01-01"), as.Date("2023-01-01"))
  )

p_forecast

p_forecast +
  facet_wrap(~.model)
```

## Accuracy

```{r}
accuracy(forecasted, incidence_df_full)
```

## 2019 only

Use fitted models to forecast for 2019 only

```{r}
incidence_df_2019 <- incidence_df_full %>%
  filter(year(date_admitted) == 2019)

forecasted_2019 <- forecast(fit_all, h = 12)

p_forecast_2019 <- forecasted_2019 %>%
  autoplot(incidence_df_2019) +
  scale_x_yearmonth(date_breaks = "3 months", date_labels = "%b", date_minor_breaks = "1 month")

p_forecast_2019

p_forecast_2019 +
  facet_wrap(~.model)

accuracy(forecasted_2019, incidence_df_2019)
```

## One-month ahead forecasting
```{r}
fit <- fit_all %>% select(auto_arima)

one_step_ahead_forecast <- function(
    model,
    horizon = 48,
    same_window_sz = TRUE,
    reestimation = FALSE,
    only_2019_test = FALSE) {
  forecast_1w_df <- incidence_df %>% mutate(col = NA)

  for (i in 1:horizon) {
    forecasted_1w <- fit %>%
      forecast(h = 1) %>%
      as_tibble() %>%
      select(date_admitted, .mean) %>%
      rename(n = .mean) %>%
      mutate(col = as.factor(1))

    forecast_1w_df %<>% bind_rows(forecasted_1w)

    fit %<>% refit(forecast_1w_df, reestimate = TRUE)
  }

  forecast_1w_df %>%
    ggplot(aes(x = date_admitted, y = n, col = col)) +
    geom_line() +
    geom_line(data = incidence_df_full %>% mutate(col = as.factor(0)))

  if (only_2019) {
    test_set <- incidence_df_full %>%
      filter(year(date_admitted) == 2019) %>%
      pull(n)
    forecasted_set <- forecast_1w_df %>%
      filter(year(date_admitted) == 2019) %>%
      pull(n)
  } else {
    test_set <- incidence_df_full %>%
      filter(year(date_admitted) > 2018) %>%
      pull(n)
    forecasted_set <- forecast_1w_df %>%
      filter(year(date_admitted) > 2018) %>%
      pull(n)
  }

  list(
    sprintf(
      "arima_1m_ahead_%s_%s",
      ifelse(reestimation, "reestimate", "no_reestimate"),
      ifelse(same_window_sz, "no_truncated", "truncated")
    ),
    sqrt(mean((test_set - forecasted_set)^2)),
    mean(abs(test_set - forecasted_set))
  )
}

system.time({
  mapply(
    one_step_ahead_forecast,
    model = fit,
    list(c(TRUE, TRUE), c(TRUE, FALSE), c(FALSE, TRUE), c(FALSE, FALSE)),
  )
})
```

## Prophet weekly

```{r}
prophet_weekly <- incidence_df_weekly %>%
  filter(year(date_admitted) < 2019) %>%
  model(
    prophet = prophet(n ~ season(period = 52, order = 10, type = "multiplicative"))
  )

prophet_weekly_forecast <- prophet_weekly %>%
  forecast(h = 208)

prophet_weekly_forecast %>%
  autoplot(incidence_df_weekly)

accuracy(prophet_weekly_forecast, incidence_df_weekly)

incidence_df_weekly_2019 <- incidence_df_weekly %>%
  filter(year(date_admitted) == 2019)

prophet_weekly_forecast_2019 <- prophet_weekly %>%
  forecast(h = 52)

prophet_weekly_forecast_2019 %>%
  autoplot(incidence_df_weekly_2019)

accuracy(prophet_weekly_forecast_2019, incidence_df_weekly_2019)
```


### With holidays
```{r fig.height=15, fig.width=15}
pub_holidays <- map(2000:2022, \(year){
  c(
    sprintf("%d-01-01", year),
    sprintf("%d-12-29", subtract(year, 1)) %>% lunR::lunar_to_gregorian(),
    sprintf("%d-03-10", year) %>% lunR::lunar_to_gregorian(),
    sprintf("%d-04-30", year),
    sprintf("%d-05-01", year),
    sprintf("%d-09-02", year)
  )
}) %>%
  list_c() %>%
  as.Date()

vn_pub_holidays <- tsibble(
  date = pub_holidays,
  holiday = rep(
    c("New Year", "Tet", "Hung King", "Reunification", "Labor Day", "Independence Day"),
    23
  ),
  lower_window = rep(c(0, 0, 0, 0, 0, 0), 23),
  upper_window = rep(c(1, 7, 1, 1, 1, 2), 23),
  index = date
)

vn_pub_holidays_train <- vn_pub_holidays %>% filter(year(date) < 2019)

incidence_df_weekly %>%
  mutate(year = year(date_admitted)) %>%
  autoplot() +
  geom_vline(data = vn_pub_holidays, aes(xintercept = date), color = "red", alpha = 0.3) +
  facet_wrap(~year, scales = "free_x")
```


```{r}
prophet_weekly <- incidence_df_weekly %>%
  filter(year(date_admitted) < 2019) %>%
  model(
    prophet = prophet(
      n ~ season(period = 52, order = 10, type = "multiplicative") +
        holiday(vn_pub_holidays_train, prior_scale = 20L)
    )
  )

prophet_weekly_forecast <- prophet_weekly %>%
  forecast(h = 208)

prophet_weekly_forecast %>%
  autoplot(incidence_df_weekly) +
  geom_vline(data = vn_pub_holidays, aes(xintercept = date), color = "red", alpha = 0.1)

accuracy(prophet_weekly_forecast, incidence_df_weekly)

incidence_df_weekly_2019 <- incidence_df_weekly %>%
  filter(year(date_admitted) == 2019)

prophet_weekly_forecast_2019 <- prophet_weekly %>%
  forecast(h = 52)

prophet_weekly_forecast_2019 %>%
  autoplot(incidence_df_weekly_2019) +
  geom_vline(data = vn_pub_holidays, aes(xintercept = date), color = "red", alpha = 0.1)

accuracy(prophet_weekly_forecast_2019, incidence_df_weekly_2019)
```


# NNAR

```{r}
nn_fit <- incidence_df %>%
  filter(year(date_admitted) < 2019) %>%
  model(nnar = NNETAR(n ~ AR(2, 1, "1 year")))

report(nn_fit)

accuracy(nn_fit)

nn_forecast <- function() {
  nn_fit %>% forecast(h = 48)
}
nn_forecast %>% autoplot(incidence_df_full)
```

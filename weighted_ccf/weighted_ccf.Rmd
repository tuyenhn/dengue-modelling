---
output: 
  html_document:
    df_print: paged
    toc: yes
    theme: journal
---

**Knitted at: `r Sys.time()`**

# Libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)

library(feasts)
library(tsibble)
library(fable.prophet)
library(fabletools)
library(fable)

library(lubridate)

library(zoo)

library(pbapply)
library(parallel)
```

# Settings
```{r}
theme_set(theme_bw())
```


# Data ingestion

```{r}
source("../data_weekly.R", chdir = TRUE)
```


## Incidence data

```{r}
ggplot(incidence_weekly_df, aes(x = date_admitted, y = n)) +
  geom_line() +
  scale_x_yearweek(
    "Date",
    breaks = yearweek(paste0(seq(2000, 2023, 1), " W01")), minor_breaks = NULL,
    labels = as.character(seq(2000, 2023, 1))
  ) +
  scale_y_continuous("Case admitted") +
  ggtitle("Dengue weekly incidence")
```

### Train val test split
```{r}
incidence_weekly_df %>%
  filter_index("2000 W01" ~ "2019 W52") %>%
  ggplot(aes(x = date_admitted, y = n)) +
  geom_line() +
  geom_vline(xintercept = as.Date(yearweek("2018 W01")), color = "red") +
  geom_vline(xintercept = as.Date(yearweek("2019 W01")), color = "red") +
  scale_x_yearweek(
    "Date",
    breaks = yearweek(paste0(seq(2000, 2020, 1), " W01")), minor_breaks = NULL,
    labels = as.character(seq(2000, 2020, 1))
  ) +
  scale_y_continuous("Case admitted") +
  ggtitle("Dengue weekly incidence")
```

# Performance checking framework

```{r}
model_perfs <- tibble(
  model = character(),
  RMSE = numeric(),
  MAE = numeric()
)

forecast_perf <- function(forecast, actual, model_name) {
  rmse <- sqrt(mean((actual - forecast)^2))
  mae <- mean(abs(actual - forecast))

  print(sprintf("Test RMSE: %f", rmse))
  print(sprintf("Test MAE: %f", mae))

  model_perfs <<- model_perfs %>%
    bind_rows(tibble(model = model_name, RMSE = rmse, MAE = mae))
}

forecast_plot_default <- function(fit_df, actual_df, plot_title) {
  ggplot(actual_df) +
    geom_line(aes(x = date_admitted, y = n)) +
    forecast_layer(fit_df) +
    scale_x_yearweek(
      "Date",
      breaks = as.Date(actual_df$date_admitted[seq(1, 52, 3)]),
      labels = as.character(actual_df$date_admitted[seq(1, 52, 3)]) %>% str_extract("W\\d+"),
      minor_breaks = NULL
    ) +
    scale_y_continuous("Incidence") +
    ggtitle(plot_title)
}
```

# Model fitting and forecast framework

```{r}
recur_miso <- function(
    train_set, test_set,
    start_indices,
    model = list(), xreg_names = NULL,
    horizon = 4,
    link = c("identity", "log"), distr = c("poisson", "nbinom"),
    cl = NULL) {
  if (!missing(cl)) {
    clusterEvalQ(cl, suppressPackageStartupMessages(library(magrittr)))
    clusterEvalQ(cl, suppressPackageStartupMessages(library(dplyr)))
    clusterEvalQ(cl, library(tscount))
    clusterExport(cl, c("train_set", "test_set"), environment())
  }
  distr <- match.arg(distr)
  link <- match.arg(link)

  pbapply::pblapply(start_indices, \(start_idx) {
    train_df <- train_set %>%
      bind_rows(test_set %>% slice_head(n = start_idx)) %>%
      tail(nrow(train_set))

    if (!is.null(xreg_names)) {
      xreg <- train_df %>%
        as_tibble() %>%
        select(all_of(xreg_names)) %>%
        as.matrix()
      newxreg <- test_set %>%
        tail(52 - start_idx) %>%
        head(horizon) %>%
        as_tibble() %>%
        select(all_of(xreg_names)) %>%
        as.matrix()
    } else {
      xreg <- NULL
      newxreg <- NULL
    }

    fit <- tsglm(train_df$n, model = model, link = link, distr = distr, xreg = xreg)
    pred <- predict(fit, horizon, newobs = NULL, newxreg = newxreg)

    tibble(
      startweek = start_idx,
      actualweek = start_idx + (1:horizon - 1),
      preds = (pred$pred),
      lowers = (pred$interval[, "lower"]),
      uppers = (pred$interval[, "upper"]),
    )
  }, cl = cl) %>% list_c()
}


forecast_layer <- function(frcst_df) {
  # frcst_df <- forecast_df
  frcst_df %<>% group_by(startweek) %>% group_split()

  layers <- lapply(
    frcst_df,
    \(df)
    c(layer(
      data = df,
      geom = "ribbon", stat = "identity", position = "identity",
      mapping = aes(x = date, ymin = lowers, ymax = uppers),
      params = list(alpha = 0.5, fill = "pink")
    ), layer(
      data = df,
      geom = "line", stat = "identity", position = "identity",
      mapping = aes(x = date, y = preds), params = list(color = "red")
    ))
  )
  layers
}
```

```{r}
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list = ls(name = env), pos = env)
}
```

# Alpha

## Accumulation function
```{r}
tsibble_rollsum <- function(df, varcol, datecol, alpha = 10) {
  alpha_df <- zoo(df %>% pull(varcol), df %>% pull(datecol))
  alpha_df %>%
    rollsum(k = alpha) %>%
    timetk::tk_tbl() %>%
    as_tsibble(index = "index")
}
```

## Hyperparameter tuning

### Function
```{r}
alpha_apply <- function(row, covarDf, varcol) {
  alpha <- row["alphas"]
  lag <- row["lags"]

  rollsum_df <-
    incidence_weekly_df %>%
    left_join(
      tsibble_rollsum(covarDf, varcol, "date", alpha = alpha),
      by = c("date_admitted" = "index")
    ) %>%
    mutate(value = lag(value, lag))

  train_df <- rollsum_df %>%
    filter_index("2000 W01" ~ "2017 W52")

  test_df <- rollsum_df %>%
    filter_index("2018 W01" ~ "2018 W04")

  fit <- glm(
    n ~ date_admitted + value,
    family = poisson(), data = train_df
  )

  forecast_perf(
    fit, test_df, sprintf("alpha=%d, lag=%d", alpha, lag),
    plot = FALSE, res_print = FALSE, res_df = TRUE
  )
}

alpha_hyper_grid <- function(covarDf, varcol) {
  # param grid
  lags <- 0:52
  alphas <- 1:52

  param_grid <- expand.grid(lags = lags, alphas = alphas)

  # cluster setup
  cl <- makeCluster(6)
  clusterEvalQ(cl, suppressPackageStartupMessages(library("tidyverse")))
  clusterEvalQ(cl, library("zoo"))
  clusterEvalQ(cl, library("tsibble"))
  clusterExport(cl, ls(globalenv()))

  # hyperparam grid tuning
  apply_res <- pbapply::pbapply(
    param_grid, 1, alpha_apply,
    cl = cl, covarDf = covarDf, varcol = varcol
  )

  # teardown
  stopCluster(cl)

  apply_res %>%
    list_c() %>%
    mutate(
      alpha = str_match(model, ".+=(\\d+).+=.+")[, 2] %>% as.numeric(),
      lag = str_match(model, ".+=.+=(\\d+)")[, 2] %>% as.numeric()
    )
}



alpha_precip_perfs <- alpha_hyper_grid(hcmc_precip_df, "precip")
alpha_temp_perfs <- alpha_hyper_grid(hcmc_temp_df, "t2m")
alpha_rh_perfs <- alpha_hyper_grid(hcmc_rh_df, "rh")

# alpha_precip_perfs %>% write_rds("./grid_perfs/alpha_precip_perfs.rds")
# alpha_temp_perfs %>% write_rds("./grid_perfs/alpha_temp_perfs.rds")
# alpha_rh_perfs %>% write_rds("./grid_perfs/alpha_rh_perfs.rds")
```

### Results

```{r}
best_precip_param <- alpha_precip_perfs %>% filter(RMSE == min(RMSE))
alpha_precip_perfs %>%
  ggplot(aes(x = lag, y = alpha)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_precip_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Accumulated weeks (alpha)") +
  ggtitle("Accumulation and lag effect on precipitation, 52-week forecast RMSE")
best_precip_param
```

```{r}
best_t2m_param <- alpha_temp_perfs %>% filter(RMSE == min(RMSE))
alpha_temp_perfs %>%
  ggplot(aes(x = lag, y = alpha)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_t2m_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Accumulated weeks (alpha)") +
  ggtitle("Accumulation and lag effect on 2m temperature, 52-week forecast RMSE")
best_t2m_param
```

```{r}
best_rh_param <- alpha_rh_perfs %>% filter(RMSE == min(RMSE))
alpha_rh_perfs %>%
  ggplot(aes(x = lag, y = alpha)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_rh_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Accumulated weeks (alpha)") +
  ggtitle("Accumulation and lag effect on relative humidity, 52-week forecast RMSE")
best_rh_param
```


## Testing

### Accumulated + lagged df
```{r}
accum_precip_df <- tsibble_rollsum(hcmc_precip_df, "precip", "date", alpha = 18) %>%
  mutate(value = lag(value, 13)) %>%
  rename(precip = value)

accum_t2m_df <- tsibble_rollsum(hcmc_temp_df, "t2m", "date", alpha = 20) %>%
  mutate(value = lag(value, 50)) %>%
  rename(t2m = value)

accum_rh_df <- tsibble_rollsum(hcmc_rh_df, "rh", "date", alpha = 6) %>%
  mutate(value = lag(value, 37)) %>%
  rename(rh = value)

accum_lagged_df <- accum_precip_df %>%
  left_join(accum_t2m_df) %>%
  left_join(accum_rh_df)
accum_lagged_df
```

```{r}
train_weekly_accum_df <- incidence_weekly_df %>%
  left_join(accum_lagged_df, by = c("date_admitted" = "index")) %>%
  filter_index("2000 W06" ~ "2017 W52")

test_weekly_accum_df <- incidence_weekly_df %>%
  left_join(accum_lagged_df, by = c("date_admitted" = "index")) %>%
  filter_index("2019 W01" ~ "2019 W04")
```

### Testing framework

```{r}
poisson_fit <- function(row) {
  formula <- row$formula
  data <- row$train
  newdata <- row$test
  base_model_name <- row$base_model_name

  fit <- glm(formula = formula, family = poisson(), data = data)

  model_name <- paste0(
    base_model_name, " + ",
    paste(names(fit$coefficients) %>% tail(-2), collapse = " + ")
  )

  forecast_perf(fit, newdata = newdata, model_name = model_name, res_df = TRUE, plot = FALSE, res_print = FALSE)
}

formulas <- c(
  n ~ date_admitted,
  n ~ date_admitted + t2m,
  n ~ date_admitted + precip,
  n ~ date_admitted + rh,
  n ~ date_admitted + t2m + precip,
  n ~ date_admitted + t2m + rh,
  n ~ date_admitted + precip + rh,
  n ~ date_admitted + t2m + precip + rh
)
settings <- list(
  list(train_weekly_weather_df, test_weekly_weather_df %>% head(4), "Poisson"),
  list(train_weekly_accum_df, test_weekly_accum_df, "Poisson accum lagged")
)

model_pars <- expand.grid(formula = formulas, setting = settings) %>%
  as_tibble() %>%
  rowid_to_column() %>%
  unnest(setting) %>%
  mutate(name = rep(c("train", "test", "base_model_name"), n() / 3)) %>%
  pivot_wider(id_cols = c(rowid, formula), values_from = setting) %>%
  mutate(base_model_name = unlist(base_model_name))

cl <- makeCluster(6)
clusterEvalQ(cl, suppressPackageStartupMessages(library("tidyverse")))
clusterExport(cl, c("forecast_perf"))

models_res <- pbapply::pbapply(model_pars, 1, poisson_fit, cl = cl) %>% list_c()

stopCluster(cl)

models_res

# models_res %>% write_csv("./grid_perfs/alpha_poisson_perfs.csv")
```

### Results comparison

```{r}
models_res %>%
  mutate(type = case_when(
    startsWith(model, "Poisson accum lagged") ~ "Poisson accum lagged",
    .default = "Poisson"
  )) %>%
  group_by(type) %>%
  mutate(mean_rmse = mean(RMSE)) %>%
  ggplot() +
  geom_col(aes(y = model, x = RMSE)) +
  geom_vline(aes(xintercept = mean_rmse), color = "red") +
  facet_wrap(~type, ncol = 1, scales = "free_y")
```


# Beta

```{r}
# ref_seq <- hcmc_temp_df %>% head(11)
#
# kernel <- dnorm(seq(-5, 5), sd = 1.1)
#
# convolve(ref_seq$t2m, kernel, type = "filter")
#
#
# beta_df <- zoo(hcmc_temp_df$t2m, hcmc_temp_df$date)
# beta_df %<>%
#   rollapply(width = 11, align = "center", FUN = convolve, y = kernel, type = "filter") %>%
#   timetk::tk_tbl() %>%
#   as_tsibble(index = "index")
# beta_df
#
#
# incidence_weekly_df %>%
#   left_join(hcmc_temp_df, by = c("date_admitted" = "date")) %>%
#   CCF(n, t2m, lag_max = 52) %>%
#   autoplot()
#
# incidence_weekly_df %>%
#   left_join(beta_df, by = c("date_admitted" = "index")) %>%
#   CCF(n, value, lag_max = 52) %>%
#   autoplot()
```

## Delay function

```{r}
tsibble_rollapply <- function(df, varcol, datecol, kernel, width = 11, align = "center") {
  beta_df <- zoo(df %>% pull(varcol), df %>% pull(datecol))
  beta_df %>%
    rollapply(
      width = width, align = "center",
      FUN = convolve, y = kernel, type = "filter"
    ) %>%
    timetk::tk_tbl() %>%
    as_tsibble(index = "index")
}
```

## Hyperparameter tuning

### Function

```{r}
beta_apply <- function(row, covarDf, varcol) {
  beta <- row["betas"]
  lag <- row["lags"]

  gauss_kernel <- dnorm(seq(-5, 5), sd = beta)

  rollapply_df <-
    incidence_weekly_df %>%
    left_join(
      tsibble_rollapply(covarDf, varcol, "date", kernel = gauss_kernel),
      by = c("date_admitted" = "index")
    ) %>%
    mutate(value = lag(value, lag))


  train_df <- rollapply_df %>%
    filter_index("2000 W01" ~ "2017 W52")

  test_df <- rollapply_df %>%
    filter_index("2018 W01" ~ "2018 W04")

  fit <- glm(
    n ~ date_admitted + value,
    family = poisson(), data = train_df
  )

  forecast_perf(
    fit, test_df, sprintf("beta=%d, lag=%d", beta * 10, lag),
    plot = FALSE, res_print = FALSE, res_df = TRUE
  )
}

beta_hyper_grid <- function(covarDf, varcol) {
  # param grid
  lags <- 0:52
  betas <- 1:30

  param_grid <- expand.grid(lags = lags, betas = betas / 10)

  # cluster setup
  cl <- makeCluster(6)
  clusterEvalQ(cl, suppressPackageStartupMessages(library("tidyverse")))
  clusterEvalQ(cl, library("zoo"))
  clusterEvalQ(cl, library("tsibble"))
  clusterExport(cl, ls(globalenv()))

  # hyperparam grid tuning
  apply_res <- pbapply::pbapply(
    param_grid, 1, beta_apply,
    cl = cl, covarDf = covarDf, varcol = varcol
  )

  # teardown
  stopCluster(cl)

  apply_res %>%
    list_c() %>%
    mutate(
      beta = str_match(model, ".+=(\\d+).+=.+")[, 2] %>% as.numeric() / 10,
      lag = str_match(model, ".+=.+=(\\d+)")[, 2] %>% as.numeric(),
    )
}




beta_precip_perfs <- beta_hyper_grid(hcmc_precip_df, "precip")
beta_temp_perfs <- beta_hyper_grid(hcmc_temp_df, "t2m")
beta_rh_perfs <- beta_hyper_grid(hcmc_rh_df, "rh")

# beta_precip_perfs %>% write_rds("./grid_perfs/beta_precip_perfs.rds")
# beta_temp_perfs %>% write_rds("./grid_perfs/beta_temp_perfs.rds")
# beta_rh_perfs %>% write_rds("./grid_perfs/beta_rh_perfs.rds")
```

### Results

```{r}
best_precip_param <- beta_precip_perfs %>% filter(RMSE == min(RMSE))
beta_precip_perfs %>%
  ggplot(aes(x = lag, y = beta)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_precip_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Gaussian standard deviation (beta)") +
  ggtitle("Delay and lag effect on precipitation, 52-week forecast RMSE")
best_precip_param
```

```{r}
best_t2m_param <- beta_temp_perfs %>% filter(RMSE == min(RMSE))
beta_temp_perfs %>%
  ggplot(aes(x = lag, y = beta)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_t2m_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Gaussian standard deviation (beta)") +
  ggtitle("Delay and lag effect on 2m temperature, 52-week forecast RMSE")
best_t2m_param
```

```{r}
best_rh_param <- beta_rh_perfs %>% filter(RMSE == min(RMSE))
beta_rh_perfs %>%
  ggplot(aes(x = lag, y = beta)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = best_rh_param, shape = 4, color = "red") +
  scale_fill_viridis_c() +
  scale_x_continuous("Lagged weeks", breaks = seq(0, 50, 10)) +
  scale_y_continuous("Gaussian standard deviation (beta)") +
  ggtitle("Delay and lag effect on relative humidity, 52-week forecast RMSE")
best_rh_param
```

## Testing

### Delayed + lagged df
```{r}
delay_precip_df <- tsibble_rollapply(
  hcmc_precip_df, "precip", "date",
  kernel = dnorm(seq(-5, 5), sd = 3)
) %>%
  mutate(value = lag(value, 13)) %>%
  rename(precip = value)

delay_t2m_df <- tsibble_rollapply(
  hcmc_temp_df, "t2m", "date",
  kernel = dnorm(seq(-5, 5), sd = 3)
) %>%
  mutate(value = lag(value, 48)) %>%
  rename(t2m = value)

delay_rh_df <- tsibble_rollapply(
  hcmc_rh_df, "rh", "date",
  kernel = dnorm(seq(-5, 5), sd = 1.8)
) %>%
  mutate(value = lag(value, 36)) %>%
  rename(rh = value)

delay_lagged_df <- delay_precip_df %>%
  left_join(delay_t2m_df) %>%
  left_join(delay_rh_df)
delay_lagged_df
```

```{r}
train_weekly_delay_df <- incidence_weekly_df %>%
  left_join(delay_lagged_df, by = c("date_admitted" = "index")) %>%
  filter_index("2000 W01" ~ "2017 W52")

test_weekly_delay_df <- incidence_weekly_df %>%
  left_join(delay_lagged_df, by = c("date_admitted" = "index")) %>%
  filter_index("2019 W01" ~ "2019 W04")
```

### Testing framework

```{r}
settings <- list(
  list(train_weekly_weather_df, test_weekly_weather_df %>% head(4), "Poisson"),
  list(train_weekly_delay_df, test_weekly_delay_df, "Poisson delayed lagged")
)

model_pars <- expand.grid(formula = formulas, setting = settings) %>%
  as_tibble() %>%
  rowid_to_column() %>%
  unnest(setting) %>%
  mutate(name = rep(c("train", "test", "base_model_name"), n() / 3)) %>%
  pivot_wider(id_cols = c(rowid, formula), values_from = setting) %>%
  mutate(base_model_name = unlist(base_model_name))

cl <- makeCluster(6)
clusterEvalQ(cl, suppressPackageStartupMessages(library("tidyverse")))
clusterExport(cl, c("forecast_perf"))

models_res <- pbapply::pbapply(model_pars, 1, poisson_fit, cl = cl) %>% list_c()

stopCluster(cl)

models_res

# models_res %>% write_csv("./grid_perfs/beta_poisson_perfs.csv")
```

### Results comparison

```{r}
models_res %>%
  mutate(type = case_when(
    startsWith(model, "Poisson delayed lagged") ~ "Poisson delayed lagged",
    .default = "Poisson"
  )) %>%
  group_by(type) %>%
  mutate(mean_rmse = mean(RMSE)) %>%
  ggplot() +
  geom_col(aes(y = model, x = RMSE)) +
  geom_vline(aes(xintercept = mean_rmse), color = "red") +
  facet_wrap(~type, ncol = 1, scales = "free_y")
```


# Alpha + Beta
```{r}
alpha_beta_apply <- function(row, covarDf, varcol) {
  alpha <- row["alphas"]
  beta <- row["betas"]
  # a_lag <- row["alpha_lags"]
  # b_lag <- row["beta_lags"]
  lag <- row["lags"]

  gauss_kernel <- dnorm(seq(-5, 5), sd = beta)

  roll_df <- incidence_weekly_df %>%
    left_join(
      tsibble_rollsum(covarDf, varcol, "date", alpha = alpha) %>%
        rename(accum_val = value),
      by = c("date_admitted" = "index")
    ) %>%
    left_join(
      tsibble_rollapply(covarDf, varcol, "date", kernel = gauss_kernel) %>%
        rename(delay_val = value),
      by = c("date_admitted" = "index")
    ) %>%
    mutate(
      accum_val = lag(accum_val, lag),
      delay_val = lag(delay_val, lag),
    )

  train_df <- roll_df %>%
    filter_index("2000 W01" ~ "2017 W52")

  test_df <- roll_df %>%
    filter_index("2018 W01" ~ "2018 W52")

  fit <- glm(
    n ~ date_admitted + accum_val + delay_val,
    family = poisson(), data = train_df
  )

  forecast_perf(
    # fit, test_df, sprintf("alpha=%d, a_lag=%d, beta=%d, b_lag=%d", alpha, a_lag, beta * 10, b_lag),
    fit, test_df, sprintf("alpha=%d, beta=%d, b_lag=%d", alpha, beta * 10, lag),
    plot = FALSE, res_print = FALSE, res_df = TRUE
  )
}

alpha_beta_hyper_grid <- function(covarDf, varcol) {
  # param grid
  alphas <- 1:52
  betas <- 1:30
  # alpha_lags <- 0:52
  # beta_lags <- 0:52
  lags <- 0:52

  param_grid <- expand.grid(
    alphas = alphas, betas = betas, lags = lags # , alpha_lags = alpha_lags, beta_lags = beta_lags
  )

  # cluster setup
  cl <- makeCluster(6)
  clusterEvalQ(cl, suppressPackageStartupMessages(library("tidyverse")))
  clusterEvalQ(cl, library("zoo"))
  clusterEvalQ(cl, library("tsibble"))
  clusterExport(cl, ls(globalenv()))

  # hyperparam grid tuning
  apply_res <- pbapply::pbapply(
    param_grid, 1, alpha_beta_apply,
    cl = cl, covarDf = covarDf, varcol = varcol
  )

  # teardown
  stopCluster(cl)

  apply_res %>%
    list_c()
}

alpha_beta_precip_perfs <- alpha_beta_hyper_grid(hcmc_precip_df, "precip")

#########################################################

# alpha_beta_precip_perfs <- alpha_hyper_grid(hcmc_precip_df, "precip")
# alpha_temp_perfs <- alpha_hyper_grid(hcmc_temp_df, "t2m")
# alpha_rh_perfs <- alpha_hyper_grid(hcmc_rh_df, "rh")
```
